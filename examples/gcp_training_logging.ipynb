{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp_modules import GCPTrainingConfig, package_and_upload_training_code, \\\n",
    "    GCPTrainingJob, GCPMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your project ID: test-mlflow-1click\n",
      "Enter the name of your MLFlow experiment: test\n",
      "Tracking uri set to https://mlflow-dot-test-mlflow-1click.ew.r.appspot.com\n"
     ]
    }
   ],
   "source": [
    "import mlflow_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Training Modules\n",
    "\n",
    "This notebook demonstrates how to use the GCP training modules to train an sklearn clustering model and log training metrics to MLFLow. The modeling done here is for the [Instacart Segmentation](https://docs.google.com/presentation/d/1tYHSvDUMDh06qA_Qxk2SWlpUG-976pAfN8cJyEHDHrs/edit?usp=sharing) case study. All modules in this notebook are from `gcp_modules.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables\n",
    "\n",
    "First, we define job level variables like GCS path to training data, output GCS bucket, if hyperparamter tuning should be performed, and the GCP project that will be used to run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = %env PROJECT_ID\n",
    "project_id = 'projects/{}'.format(project_name)\n",
    "experiment_name = %env EXPERIMENT_NAME\n",
    "run_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tuning = True\n",
    "training_dir = './training' # local path to the training module to be packaged using setuptools\n",
    "gcs_output_bucket = '{}-training-data'.format(project_name)\n",
    "gcs_package_prefix = 'training/sklearn_clustering_test/{}.tar.gz'.format(experiment_name)\n",
    "gcs_training_data_dir = 'gs://{}-training-data/data/instacart'.format(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "GCP AI Platform supports training via python training scripts or by custom docker containers. The current modules support only trainig by script. GCP supports most major frameworks for script training (Scikit-learn, Tensorflow, PyTorch, Keras, XGBoost).\n",
    "\n",
    "In script training, users have to package their script/code as a python package. This package has to then be uploaded to GCS and the GCS path specified when submitting the training job. The `./training` directory has been provided to show what the structure of the directory should look like. The training script alone is also made available at the bottom of this notebook.\n",
    "\n",
    "We will use a function (`package_and_upload_training_code`) to facilitate building the package and uploading it to GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, put the path to your ml-test service key json you generated during setup in the cell below. It will set an environmental variable that will allow you to authenticate to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-mlflow-1click-training-data\n",
      "training_packages/training_package.tar.gz\n",
      "File test-mlflow-1click-training-data uploaded to ./training_package.tar.gz/training_packages/training_package.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "package_and_upload_training_code(training_dir, gcs_output_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine your file in [GCS browser](https://console.cloud.google.com/storage/browser/raybeam-training-ml-setup-test;tab=objects?forceOnBucketsSortingFiltering=false&project=raybeam-training&prefix=&forceOnObjectsSortingFiltering=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Submit Training Job\n",
    "Next, we will define a training job configuration to be used for training and we will submit the training job to GCP. First, we have to define a dict of hyperparamters to use for training. These hyperparamters must be command line arguments that the training script must accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'num_clusters': 4, 'reduce': True, 'num_dimensions': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GCPTrainingConfig` contains a set of default configurations for the job (e.g. instance type, # instances, # workers, region...etc). These can all be overridden by passing another value when initializing the module (e.g. `GCPTrainingConfig(..., masterType='complex_model_m')`)\n",
    "\n",
    "For more information on training job configurations: https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GCPTrainingConfig(gcs_output_bucket, gcs_training_data_dir)\n",
    "config.set_hyperparameters(metric_name='ssd', hyperparams=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the json configuration as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaleTier': 'CUSTOM',\n",
       " 'masterType': 'complex_model_m',\n",
       " 'workerType': 'complex_model_m',\n",
       " 'parameterServerType': 'large_model',\n",
       " 'workerCount': 1,\n",
       " 'parameterServerCount': 1,\n",
       " 'packageUris': ['gs://test-mlflow-1click-training-data/training_packages/training_package.tar.gz'],\n",
       " 'pythonModule': 'trainer.task',\n",
       " 'args': ['--train', 'gs://test-mlflow-1click-training-data/data/instacart'],\n",
       " 'region': 'us-central1',\n",
       " 'jobDir': 'gs://test-mlflow-1click-training-data/artifacts',\n",
       " 'runtimeVersion': '2.2',\n",
       " 'pythonVersion': '3.7',\n",
       " 'scheduling': {'maxWaitTime': '3600s', 'maxRunningTime': '14400s'},\n",
       " 'hyperparameters': {'goal': 'MINIMIZE',\n",
       "  'hyperparameterMetricTag': 'ssd',\n",
       "  'maxTrials': 1,\n",
       "  'maxParallelTrials': 1,\n",
       "  'params': [{'parameterName': 'num_clusters',\n",
       "    'type': 'CATEGORICAL',\n",
       "    'categoricalValues': ['4']},\n",
       "   {'parameterName': 'reduce',\n",
       "    'type': 'CATEGORICAL',\n",
       "    'categoricalValues': ['True']},\n",
       "   {'parameterName': 'num_dimensions',\n",
       "    'type': 'CATEGORICAL',\n",
       "    'categoricalValues': ['2']}]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GCPTrainingJob` takes the configuration created above and will use it to submit and track a training job. `submit_job()` will submit the job and return the response JSON. `track_job()` will monitor the job until a success or failure response is given, at which point it will return the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobId': 'training_test_2021_09_13_14_22_46',\n",
       " 'trainingInput': {'scaleTier': 'CUSTOM',\n",
       "  'masterType': 'complex_model_m',\n",
       "  'workerType': 'complex_model_m',\n",
       "  'parameterServerType': 'large_model',\n",
       "  'workerCount': '1',\n",
       "  'parameterServerCount': '1',\n",
       "  'packageUris': ['gs://test-mlflow-1click-training-data/training_packages/training_package.tar.gz'],\n",
       "  'pythonModule': 'trainer.task',\n",
       "  'args': ['--train', 'gs://test-mlflow-1click-training-data/data/instacart'],\n",
       "  'hyperparameters': {'goal': 'MINIMIZE',\n",
       "   'params': [{'parameterName': 'num_clusters',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['4']},\n",
       "    {'parameterName': 'reduce',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['True']},\n",
       "    {'parameterName': 'num_dimensions',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['2']}],\n",
       "   'maxTrials': 1,\n",
       "   'maxParallelTrials': 1,\n",
       "   'hyperparameterMetricTag': 'ssd'},\n",
       "  'region': 'us-central1',\n",
       "  'runtimeVersion': '2.2',\n",
       "  'jobDir': 'gs://test-mlflow-1click-training-data/artifacts/training_test_2021_09_13_14_22_46',\n",
       "  'pythonVersion': '3.7',\n",
       "  'scheduling': {'maxRunningTime': '14400s', 'maxWaitTime': '3600s'}},\n",
       " 'createTime': '2021-09-13T21:34:42Z',\n",
       " 'state': 'QUEUED',\n",
       " 'trainingOutput': {'isHyperparameterTuningJob': True},\n",
       " 'etag': 'obwpB/n2vHs='}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = GCPTrainingJob(gcp_project_name=project_name, job_name='training_test', training_config=config)\n",
    "job.submit_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2021-09-13 14:34:47.315461, State: RUNNING\n",
      "Time: 2021-09-13 14:35:17.453620, State: RUNNING\n",
      "Time: 2021-09-13 14:35:47.628527, State: RUNNING\n",
      "Time: 2021-09-13 14:36:17.763402, State: RUNNING\n",
      "Time: 2021-09-13 14:36:47.946263, State: RUNNING\n",
      "Time: 2021-09-13 14:37:18.079878, State: RUNNING\n",
      "Time: 2021-09-13 14:37:48.203279, State: RUNNING\n",
      "Time: 2021-09-13 14:38:18.339221, State: RUNNING\n",
      "Time: 2021-09-13 14:38:48.478325, State: RUNNING\n",
      "Time: 2021-09-13 14:39:18.623938, State: RUNNING\n",
      "Time: 2021-09-13 14:39:48.759153, State: RUNNING\n",
      "Time: 2021-09-13 14:40:18.866107, State: RUNNING\n",
      "Time: 2021-09-13 14:40:48.985790, State: RUNNING\n",
      "Time: 2021-09-13 14:41:19.109592, State: RUNNING\n",
      "Time: 2021-09-13 14:41:49.236487, State: RUNNING\n",
      "Time: 2021-09-13 14:42:19.363802, State: RUNNING\n",
      "Time: 2021-09-13 14:42:49.487727, State: RUNNING\n",
      "Time: 2021-09-13 14:43:19.622152, State: RUNNING\n",
      "Time: 2021-09-13 14:43:49.741313, State: RUNNING\n",
      "Time: 2021-09-13 14:44:19.862709, State: RUNNING\n",
      "Time: 2021-09-13 14:44:49.983135, State: RUNNING\n",
      "Time: 2021-09-13 14:45:20.368773, State: RUNNING\n",
      "Time: 2021-09-13 14:45:50.499725, State: RUNNING\n",
      "Time: 2021-09-13 14:46:20.618884, State: RUNNING\n",
      "Time: 2021-09-13 14:46:50.743951, State: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'jobId': 'training_test_2021_09_13_14_22_46',\n",
       " 'trainingInput': {'scaleTier': 'CUSTOM',\n",
       "  'masterType': 'complex_model_m',\n",
       "  'workerType': 'complex_model_m',\n",
       "  'parameterServerType': 'large_model',\n",
       "  'workerCount': '1',\n",
       "  'parameterServerCount': '1',\n",
       "  'packageUris': ['gs://test-mlflow-1click-training-data/training_packages/training_package.tar.gz'],\n",
       "  'pythonModule': 'trainer.task',\n",
       "  'args': ['--train', 'gs://test-mlflow-1click-training-data/data/instacart'],\n",
       "  'hyperparameters': {'goal': 'MINIMIZE',\n",
       "   'params': [{'parameterName': 'num_clusters',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['4']},\n",
       "    {'parameterName': 'reduce',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['True']},\n",
       "    {'parameterName': 'num_dimensions',\n",
       "     'type': 'CATEGORICAL',\n",
       "     'categoricalValues': ['2']}],\n",
       "   'maxTrials': 1,\n",
       "   'maxParallelTrials': 1,\n",
       "   'hyperparameterMetricTag': 'ssd'},\n",
       "  'region': 'us-central1',\n",
       "  'runtimeVersion': '2.2',\n",
       "  'jobDir': 'gs://test-mlflow-1click-training-data/artifacts/training_test_2021_09_13_14_22_46',\n",
       "  'pythonVersion': '3.7',\n",
       "  'scheduling': {'maxRunningTime': '14400s', 'maxWaitTime': '3600s'}},\n",
       " 'createTime': '2021-09-13T21:34:42Z',\n",
       " 'startTime': '2021-09-13T21:34:45Z',\n",
       " 'endTime': '2021-09-13T21:46:43Z',\n",
       " 'state': 'SUCCEEDED',\n",
       " 'trainingOutput': {'completedTrialCount': '1',\n",
       "  'trials': [{'trialId': '1',\n",
       "    'hyperparameters': {'num_clusters': '4',\n",
       "     'num_dimensions': '2',\n",
       "     'reduce': 'True'},\n",
       "    'finalMetric': {'trainingStep': '1', 'objectiveValue': 170016.4248170374},\n",
       "    'startTime': '2021-09-13T21:34:50.462579186Z',\n",
       "    'endTime': '2021-09-13T21:44:59Z',\n",
       "    'state': 'SUCCEEDED'}],\n",
       "  'consumedMLUnits': 0.55,\n",
       "  'isHyperparameterTuningJob': True,\n",
       "  'hyperparameterMetricTag': 'ssd'},\n",
       " 'etag': 'OlLNrU2yUXQ='}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.track_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view your job on the AI Platform [console](https://console.cloud.google.com/ai-platform/jobs?project=raybeam-training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of any ML development is tracking different experiments, trials, paramters, and metrics. There are many tools available to do this but we will be using [MLFlow Tracking](https://www.mlflow.org/docs/latest/tracking.html).\n",
    "\n",
    "`GCPMetricsLogger` will log the training metrics outputted by the training script. Each script has to log a metric using the `hypertune` package. In the training script below, we use the `report_hyperparameter_tuning_metric` function to log metrics. These metrics will be logged to MLFlow. `GCPMetricsLogger` will initialize an MLFlow client which will log parameters and metrics to a database (a postgres database in this case). \n",
    "\n",
    "To log metrics to MLFlow, we first create an instance of `GCPMetricsLogger` and initialize an mlflow experiment. The `experiment_name` does not have to be unique - there can be many runs under the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "metrics_logger = GCPMetricsLogger(os.environ[\"TRACKING_URI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mlflow-dot-test-mlflow-1click.ew.r.appspot.com\r\n"
     ]
    }
   ],
   "source": [
    "!echo $TRACKING_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the instance of `GCPTrainingJob` we created earlier. The training job must have been successfully complete in order to log the metrics successfully "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_logger.log_training_metrics(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow has a UI that displays details about experiments and the various runs within an experiment. Because github is unable to display images in notebooks, screenshots of the UI have been documented in this [markdown file](mlflow_ui.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "The process for hyperparamter tuning is very similar to training. The only difference is that users will be expected to provide the GCP hyperparameter tuning configuration explicitly here. This configuration must be passed to the `tuning_hyperparams` option of the `GCPTrainingConfig.set_hyperparameters` function and `run_tuning` must also be set to `True`.\n",
    "\n",
    "For more information on this tuning configuration: https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#HyperparameterSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_hyperparameters = [\n",
    "    {\n",
    "        'parameterName':'num_clusters',\n",
    "        'type':'INTEGER',\n",
    "        'minValue': 1,\n",
    "        'maxValue': 20,\n",
    "        'scaleType': 'UNIT_LINEAR_SCALE'\n",
    "    },\n",
    "    {\n",
    "        'parameterName':'num_dimensions',\n",
    "        'type':'INTEGER',\n",
    "        'minValue': 1,\n",
    "        'maxValue': 3,\n",
    "        'scaleType': 'UNIT_REVERSE_LOG_SCALE'\n",
    "    },\n",
    "    {\n",
    "        'parameterName':'reduce',\n",
    "        'type':'CATEGORICAL',\n",
    "        'categoricalValues': ['True', 'False']\n",
    "    },\n",
    "]\n",
    "\n",
    "hyperparams = {\n",
    "    'goal': 'MINIMIZE',\n",
    "    'hyperparameterMetricTag': 'ssd',\n",
    "    'maxTrials': 5,\n",
    "    'maxParallelTrials': 5,\n",
    "    'enableTrialEarlyStopping': True,\n",
    "    'params': tuning_hyperparameters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GCPTrainingConfig(gcs_output_bucket, gcs_training_data_dir)\n",
    "config.set_hyperparameters(metric_name='ssd', run_tuning=True, tuning_hyperparams=hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = GCPTrainingJob(gcp_project_name=project_name, job_name='training_test_hpt_v2', training_config=config)\n",
    "job.submit_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.track_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally metrics are logged using the same process as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_logger = GCPMetricsLogger()\n",
    "metrics_logger.initialize_mlflow(\"hypertune\")\n",
    "metrics_logger.log_training_metrics(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "The full training script used to run this model has been provided below. The training script must fulfill certain requirements:\n",
    "\n",
    "1. The script must accept hyperparamters as command line arguments through `argparse`\n",
    "1. The script must report training metrics using the `hypertune` library\n",
    "1. The script must save the trained model in pkl, joblib, or bst format (more info here: https://cloud.google.com/ai-platform/training/docs/getting-started-scikit-xgboost#model_file_naming_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import hypertune\n",
    "\n",
    "\n",
    "def log_normalize_features(data_df, method='skewed_only', skew_threshold=1):\n",
    "    if not isinstance(data_df, pd.DataFrame):\n",
    "        data_df = pd.DataFrame(data_df)\n",
    "    if method == 'none':\n",
    "        return data_df\n",
    "    elif method == 'skewed_only':\n",
    "        skewness = stats.skew(data_df)\n",
    "        log_data_df = data_df.copy()\n",
    "        for col, skew in zip(data_df.columns, skewness):\n",
    "            if abs(skew) >= skew_threshold:\n",
    "                log_data_df[col] = np.log(data_df[col])\n",
    "        return log_data_df\n",
    "    elif method == 'all':\n",
    "        return pd.DataFrame(np.log(data_df), columns=data_df.columns, index=data_df.index)\n",
    "    else:\n",
    "        raise ValueError('Unknown normalize method {}'.format(method))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    #Job Parameters\n",
    "    parser.add_argument('--standardize', type=bool, default=True)\n",
    "    parser.add_argument('--normalize', type=str, default='skewed_only')\n",
    "    parser.add_argument('--skew_threshold', type=float, default=1.0)\n",
    "    parser.add_argument('--reduce', type=bool, default=False)\n",
    "    parser.add_argument('--num_dimensions', type=int, default=50)\n",
    "    parser.add_argument('--num_clusters', type=int, required=True)\n",
    "\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--job-dir', type=str)\n",
    "    parser.add_argument('--model-dir', type=str, default=\"./\")\n",
    "    parser.add_argument('--train', type=str, help='GCS storage directory path', required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    local_dir = os.path.basename(args.train)\n",
    "    if args.train[-1] == '/':\n",
    "        local_dir = os.path.basename(args.train[:-1])\n",
    "    subprocess.check_call(['gsutil', 'cp', '-r', args.train, './'], stderr=sys.stdout)\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [os.path.join(args.train, file) for file in os.listdir(local_dir)]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "\n",
    "    raw_data = [pd.read_csv(file, header=None) for file in input_files]\n",
    "    concat_data = pd.concat(raw_data)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    normalizer = FunctionTransformer(log_normalize_features,\n",
    "                                     kw_args={'method': args.normalize,\n",
    "                                              'skew_threshold': args.skew_threshold})\n",
    "\n",
    "    steps = [('normalize', normalizer), ('scale', scaler)]\n",
    "    if args.reduce:\n",
    "        num_dimensions = min(args.num_dimensions, concat_data.shape[1])\n",
    "        reducer = PCA(n_components=num_dimensions)\n",
    "        steps.append(('reduce', reducer))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=args.num_clusters)\n",
    "    steps.append(('cluster', kmeans))\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    pipeline.fit(concat_data)\n",
    "    ssd = pipeline['cluster'].inertia_\n",
    "    print('Training:SSD = {};'.format(ssd))\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "                                            hyperparameter_metric_tag='ssd',\n",
    "                                            metric_value=ssd)\n",
    "    joblib.dump(pipeline, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    subprocess.check_call(['gsutil', 'cp', 'model.joblib', args.job_dir], stderr=sys.stdout)\n",
    "    print(\"saved model!\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1abd0e4af6a3016f15bbfe0f3eb6e904131b64d0c7b0887b3b5958ac6b3fa1a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
